{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 練習以gensim訓練詞向量：\n",
    "## 利用wiki data，以skip-gram model來建立word2vec向量模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "import json\n",
    "import pandas\n",
    "import pickle\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from gensim.corpora import WikiCorpus\n",
    "from gensim import models\n",
    "import logging\n",
    "from gensim.models import word2vec\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import logging\n",
    "import sys\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 取得語料\n",
    "### 1-1 取得中文維基數據，本次練習是採用 2018/12/20 的資料。（https://zh.wikipedia.org/wiki/Wikipedia:%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8B%E8%BD%BD）\n",
    "### 1-2 將下載後的維基數據置於與專案同個目錄，再使用wiki_to_txt.py從 xml 中提取出維基文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'zhwiki-20181220-pages-articles.xml.bz2'\n",
    "f = open('zhwiki.txt', encoding='utf8', mode='w')\n",
    "wiki =  gensim.corpora.WikiCorpus(input_file, lemmatize=False, dictionary={})\n",
    "for text in wiki.get_texts():\n",
    "    str_line = ' '.join(text)\n",
    "    f.write(str_line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"name\": \"Traditional Chinese to Simplified Chinese\",\n",
    "  \"segmentation\": {\n",
    "    \"type\": \"mmseg\",\n",
    "    \"dict\": {\n",
    "      \"type\": \"ocd\",\n",
    "      \"file\": \"TSPhrases.ocd\"\n",
    "    }\n",
    "  },\n",
    "  \"conversion_chain\": [{\n",
    "    \"dict\": {\n",
    "      \"type\": \"group\",\n",
    "      \"dicts\": [{\n",
    "        \"type\": \"ocd\",\n",
    "        \"file\": \"TSPhrases.ocd\"\n",
    "      }, {\n",
    "        \"type\": \"ocd\",\n",
    "        \"file\": \"TSCharacters.ocd\"\n",
    "      }]\n",
    "    }\n",
    "  }]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 使用 OpenCC 將維基文章統一轉換為繁體中文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opencc -i zhwiki.txt -o zhwiki_tw.txt -c s2twp.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  使用jieba 對文本斷詞，並去除停用詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "    # jieba custom setting.\n",
    "    jieba.set_dictionary('/Users/airmac/Desktop/PTT/crawler/userdictionary/dict2_PTT.txt')\n",
    "\n",
    "    # load stopwords set\n",
    "    stopword_set = set()\n",
    "    with open('/Users/airmac/Desktop/PTT/crawler/stopwords.txt','r', encoding='utf-8') as stopwords:\n",
    "        for stopword in stopwords:\n",
    "            stopword_set.add(stopword.strip('\\n'))\n",
    "\n",
    "    output = open('wiki_seg.txt', 'w', encoding='utf-8')\n",
    "    with open('zhwiki_tw.txt', 'r', encoding='utf-8') as content :\n",
    "        for texts_num, line in enumerate(content):\n",
    "            line = line.strip('\\n')\n",
    "            words = jieba.cut(line, cut_all=False)\n",
    "            for word in words:\n",
    "                if word not in stopword_set:\n",
    "                    output.write(word + ' ')\n",
    "            output.write('\\n')\n",
    "\n",
    "            if (texts_num + 1) % 10000 == 0:\n",
    "                logging.info(\"已完成前 %d 行的斷詞\" % (texts_num + 1))\n",
    "    output.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 使用gensim 的 word2vec 模型進行訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-22 22:01:52,765 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-05-22 22:01:52,770 : INFO : collecting all words and their counts\n",
      "2019-05-22 22:01:52,778 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-05-22 22:01:52,866 : INFO : collected 33301 word types from a corpus of 102451 raw words and 60 sentences\n",
      "2019-05-22 22:01:52,868 : INFO : Loading a fresh vocabulary\n",
      "2019-05-22 22:01:52,919 : INFO : effective_min_count=3 retains 5983 unique words (17% of original 33301, drops 27318)\n",
      "2019-05-22 22:01:52,920 : INFO : effective_min_count=3 leaves 71078 word corpus (69% of original 102451, drops 31373)\n",
      "2019-05-22 22:01:52,951 : INFO : deleting the raw counts dictionary of 33301 items\n",
      "2019-05-22 22:01:52,955 : INFO : sample=0.001 downsamples 21 most-common words\n",
      "2019-05-22 22:01:52,960 : INFO : downsampling leaves estimated 68690 word corpus (96.6% of prior 71078)\n",
      "2019-05-22 22:01:52,994 : INFO : estimated required memory for 5983 words and 250 dimensions: 14957500 bytes\n",
      "2019-05-22 22:01:52,996 : INFO : resetting layer weights\n",
      "2019-05-22 22:01:53,179 : INFO : training model with 3 workers on 5983 vocabulary and 250 features, using sg=1 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-05-22 22:01:54,255 : INFO : EPOCH 1 - PROGRESS: at 90.00% examples, 50287 words/s, in_qsize 3, out_qsize 0\n",
      "2019-05-22 22:01:54,341 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-22 22:01:54,418 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-22 22:01:54,431 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-22 22:01:54,433 : INFO : EPOCH - 1 : training on 102451 raw words (68607 effective words) took 1.2s, 54897 effective words/s\n",
      "2019-05-22 22:01:55,467 : INFO : EPOCH 2 - PROGRESS: at 91.67% examples, 57833 words/s, in_qsize 2, out_qsize 1\n",
      "2019-05-22 22:01:55,469 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-22 22:01:55,581 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-22 22:01:55,588 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-22 22:01:55,589 : INFO : EPOCH - 2 : training on 102451 raw words (68708 effective words) took 1.2s, 59635 effective words/s\n",
      "2019-05-22 22:01:56,674 : INFO : EPOCH 3 - PROGRESS: at 73.33% examples, 38248 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-22 22:01:56,902 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-22 22:01:56,957 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-22 22:01:56,963 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-22 22:01:56,964 : INFO : EPOCH - 3 : training on 102451 raw words (68678 effective words) took 1.4s, 50068 effective words/s\n",
      "2019-05-22 22:01:57,988 : INFO : EPOCH 4 - PROGRESS: at 91.67% examples, 58560 words/s, in_qsize 2, out_qsize 1\n",
      "2019-05-22 22:01:57,990 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-22 22:01:58,048 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-22 22:01:58,060 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-22 22:01:58,061 : INFO : EPOCH - 4 : training on 102451 raw words (68717 effective words) took 1.1s, 62989 effective words/s\n",
      "2019-05-22 22:01:59,027 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-22 22:01:59,083 : INFO : EPOCH 5 - PROGRESS: at 98.33% examples, 64132 words/s, in_qsize 1, out_qsize 1\n",
      "2019-05-22 22:01:59,085 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-22 22:01:59,087 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-22 22:01:59,090 : INFO : EPOCH - 5 : training on 102451 raw words (68663 effective words) took 1.0s, 67162 effective words/s\n",
      "2019-05-22 22:01:59,092 : INFO : training on a 512255 raw words (343373 effective words) took 5.9s, 58084 effective words/s\n",
      "2019-05-22 22:01:59,094 : INFO : saving Word2Vec object under word2vec.model_wiki, separately None\n",
      "2019-05-22 22:01:59,096 : INFO : not storing attribute vectors_norm\n",
      "2019-05-22 22:01:59,098 : INFO : not storing attribute cum_table\n",
      "2019-05-22 22:01:59,421 : INFO : saved word2vec.model_wiki\n",
      "2019-05-22 22:01:59,425 : INFO : loading Word2Vec object from word2vec.model_wiki\n",
      "2019-05-22 22:01:59,651 : INFO : loading wv recursively from word2vec.model_wiki.wv.* with mmap=None\n",
      "2019-05-22 22:01:59,652 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-05-22 22:01:59,655 : INFO : loading vocabulary recursively from word2vec.model_wiki.vocabulary.* with mmap=None\n",
      "2019-05-22 22:01:59,657 : INFO : loading trainables recursively from word2vec.model_wiki.trainables.* with mmap=None\n",
      "2019-05-22 22:01:59,662 : INFO : setting ignored attribute cum_table to None\n",
      "2019-05-22 22:01:59,664 : INFO : loaded word2vec.model_wiki\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    sentences = word2vec.LineSentence('./wiki_seg.txt') #specify file name\n",
    "    model_SG = word2vec.Word2Vec(sentences, size=250, window = 10, sg = 1, min_count = 3)\n",
    "\n",
    "    #保存模型，供日後使用\n",
    "    model_SG.save(\"word2vec.model_wiki\")\n",
    "\n",
    "    #模型讀取方式\n",
    "    model_SG = word2vec.Word2Vec.load(\"word2vec.model_wiki\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 測試訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-23 21:00:20,461 : INFO : loading Word2Vec object from word2vec.model_wiki\n",
      "2019-05-23 21:00:20,708 : INFO : loading wv recursively from word2vec.model_wiki.wv.* with mmap=None\n",
      "2019-05-23 21:00:20,709 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-05-23 21:00:20,714 : INFO : loading vocabulary recursively from word2vec.model_wiki.vocabulary.* with mmap=None\n",
      "2019-05-23 21:00:20,716 : INFO : loading trainables recursively from word2vec.model_wiki.trainables.* with mmap=None\n",
      "2019-05-23 21:00:20,718 : INFO : setting ignored attribute cum_table to None\n",
      "2019-05-23 21:00:20,721 : INFO : loaded word2vec.model_wiki\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提供 3 種測試模式\n",
      "\n",
      "輸入一個詞，則去尋找前一百個該詞的相似詞\n",
      "輸入兩個詞，則去計算兩個詞的餘弦相似度\n",
      "輸入三個詞，進行類比推理\n",
      "維基\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "2019-05-23 21:00:33,550 : INFO : precomputing L2-norms of word weight vectors\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "相似詞前 100 排序\n",
      "鮑姆,0.9982941150665283\n",
      "塔能,0.9981815814971924\n",
      "專利,0.997909665107727\n",
      "奇異,0.9978553056716919\n",
      "傳送,0.9976803064346313\n",
      "坎寧安,0.9976129531860352\n",
      "下載,0.9974729418754578\n",
      "執行緒,0.9974358081817627\n",
      "git,0.9973118305206299\n",
      "原生,0.9972920417785645\n",
      "標,0.9972141981124878\n",
      "搜尋,0.9971519112586975\n",
      "磁帶機,0.9970462918281555\n",
      "相容性,0.9967495203018188\n",
      "power,0.996607780456543\n",
      "group,0.9965818524360657\n",
      "芬蘭赫爾辛,0.9965640902519226\n",
      "python,0.9965051412582397\n",
      "赫爾辛,0.9964991807937622\n",
      "用來,0.9964420795440674\n",
      "一份,0.9963663816452026\n",
      "用語,0.9963191747665405\n",
      "讀,0.9962285757064819\n",
      "專訪,0.996159553527832\n",
      "提議,0.9960776567459106\n",
      "起訴,0.995819628238678\n",
      "分享,0.9955840110778809\n",
      "路徑,0.9955559968948364\n",
      "應器,0.995527446269989\n",
      "美國國家,0.9954522848129272\n",
      "文學手,0.9951719045639038\n",
      "environment,0.995078980922699\n",
      "保證,0.9950613379478455\n",
      "理器,0.9950302839279175\n",
      "理察,0.9950070381164551\n",
      "指南,0.9949942827224731\n",
      "md,0.9949941635131836\n",
      "人選,0.99495530128479\n",
      "辦法,0.9949389696121216\n",
      "貝爾,0.9949385523796082\n",
      "屬,0.9949334263801575\n",
      "基大學,0.99492347240448\n",
      "ic,0.9949181079864502\n",
      "語法,0.994899570941925\n",
      "冊,0.9948554039001465\n",
      "縮,0.9948363304138184\n",
      "right,0.994832456111908\n",
      "nova,0.9948011636734009\n",
      "符,0.9947088956832886\n",
      "螢幕,0.9946956634521484\n",
      "幀,0.9946808815002441\n",
      "office,0.9946743249893188\n",
      "腦,0.9946178197860718\n",
      "視訊,0.9945335984230042\n",
      "佇列,0.9945131540298462\n",
      "多方面,0.9943578243255615\n",
      "創立者,0.994349479675293\n",
      "移除,0.9942220449447632\n",
      "傳記,0.9942188858985901\n",
      "賣,0.9941517114639282\n",
      "點,0.9941226243972778\n",
      "版權,0.994106113910675\n",
      "圍紀,0.9940536022186279\n",
      "根基,0.9939604997634888\n",
      "協定,0.9939593076705933\n",
      "debunking,0.9938493967056274\n",
      "工會,0.9938369989395142\n",
      "許多種,0.9937776327133179\n",
      "而來,0.9936307072639465\n",
      "此為,0.9935781955718994\n",
      "url,0.9934848546981812\n",
      "條款,0.9933880567550659\n",
      "馮諾,0.9933698773384094\n",
      "黑客,0.9932782649993896\n",
      "托勒密,0.9932543039321899\n",
      "是關,0.9930625557899475\n",
      "丹尼斯,0.9930038452148438\n",
      "教科書,0.9929787516593933\n",
      "學作,0.9929201006889343\n",
      "更名,0.9928343892097473\n",
      "瓦茲本,0.992814302444458\n",
      "simula,0.992810845375061\n",
      "通常會,0.9927802085876465\n",
      "沃德,0.9927790760993958\n",
      "wacom,0.9926408529281616\n",
      "一代,0.9926127195358276\n",
      "單元,0.9925980567932129\n",
      "學分,0.9925013184547424\n",
      "移植,0.9923980832099915\n",
      "學時,0.9923934936523438\n",
      "geographic,0.9923598766326904\n",
      "快點,0.9922834038734436\n",
      "small,0.9922333359718323\n",
      "並不,0.9922270178794861\n",
      "認證,0.9922029972076416\n",
      "安德魯,0.9921829700469971\n",
      "當年,0.9921236038208008\n",
      "有益,0.9920326471328735\n",
      "預設,0.9920238852500916\n",
      "dec,0.9919708967208862\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\tlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\tmodel_SG = word2vec.Word2Vec.load(\"word2vec.model_wiki\")\n",
    "\n",
    "\tprint(\"提供 3 種測試模式\\n\")\n",
    "\tprint(\"輸入一個詞，則去尋找前一百個該詞的相似詞\")\n",
    "\tprint(\"輸入兩個詞，則去計算兩個詞的餘弦相似度\")\n",
    "\tprint(\"輸入三個詞，進行類比推理\")\n",
    "\n",
    "\twhile True:\n",
    "\t\ttry:\n",
    "\t\t\tquery = input()\n",
    "\t\t\tq_list = query.split()\n",
    "\n",
    "\t\t\tif len(q_list) == 1:\n",
    "\t\t\t\tprint(\"相似詞前 100 排序\")\n",
    "\t\t\t\tres = model_SG.most_similar(q_list[0],topn = 100)\n",
    "\t\t\t\tfor item in res:\n",
    "\t\t\t\t\tprint(item[0]+\",\"+str(item[1]))\n",
    "\n",
    "\t\t\telif len(q_list) == 2:\n",
    "\t\t\t\tprint(\"計算 Cosine 相似度\")\n",
    "\t\t\t\tres = model_SG.similarity(q_list[0],q_list[1])\n",
    "\t\t\t\tprint(res)\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(\"%s之於%s，如%s之於\" % (q_list[0],q_list[2],q_list[1]))\n",
    "\t\t\t\tres = model_SG.most_similar([q_list[0],q_list[1]], [q_list[2]], topn= 100)\n",
    "\t\t\t\tfor item in res:\n",
    "\t\t\t\t\tprint(item[0]+\",\"+str(item[1]))\n",
    "\t\t\tprint(\"----------------------------\")\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(repr(e))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tmain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 分析結果提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SG = word2vec.Word2Vec.load(\"word2vec.model_wiki\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
